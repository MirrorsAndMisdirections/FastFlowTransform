from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Annotated

import typer

# Engines supported by the skeleton generator.
_SUPPORTED_ENGINES = {
    "duckdb",
    "postgres",
    "bigquery",
    "bigquery_bf",
    "databricks_spark",
    "snowflake_snowpark",
}


@dataclass(frozen=True)
class _InitContext:
    project_dir: Path
    project_name: str
    profile_name: str
    engine: str


def _build_profiles_yaml(ctx: _InitContext) -> str:
    engine_block = {
        "duckdb": [
            "  # DuckDB profile example. See docs/Profiles.md#duckdb for details.",
            "  duckdb:",
            "    path: \"{{ env('FF_DUCKDB_PATH', '.local/dev.duckdb') }}\"  # Path to your DuckDB database file.",  # Noqa E501
        ],
        "postgres": [
            "  # Postgres profile example. See docs/Profiles.md#postgres for required keys.",
            "  postgres:",
            "    dsn: \"{{ env('FF_PG_DSN') }}\"  # Full Postgres DSN, e.g. postgresql://user:pass@host/db",
            "    db_schema: \"{{ env('FF_PG_SCHEMA', 'analytics') }}\"",
        ],
        "bigquery": [
            "  # BigQuery profile example. See docs/Profiles.md#bigquery.",
            "  bigquery:",
            "    project: \"{{ env('FF_BQ_PROJECT') }}\"  # GCP project id.",
            "    dataset: \"{{ env('FF_BQ_DATASET') }}\"  # Target dataset for models.",
            "    location: US  # Update to match your dataset location.",
        ],
        "bigquery_bf": [
            "  # BigQuery BigFrames profile example. See docs/Profiles.md#bigquery.",
            "  bigquery_bf:",
            "    project: \"{{ env('FF_BQ_PROJECT') }}\"",
            "    dataset: \"{{ env('FF_BQ_DATASET') }}\"",
            "    location: US",
        ],
        "databricks_spark": [
            "  # Databricks Spark profile example. See docs/Profiles.md#databricks-spark.",
            "  databricks_spark:",
            "    master: \"{{ env('FF_SPARK_MASTER') }}\"  # e.g. spark://host:7077 or a Databricks cluster URL.",  # Noqa E501
            "    app_name: \"{{ env('FF_SPARK_APP_NAME', 'fft-project') }}\"",
            "    warehouse_dir: \"{{ env('FF_SPARK_WAREHOUSE', '/tmp/fft-warehouse') }}\"",
            "    use_hive_metastore: false",
        ],
        "snowflake_snowpark": [
            "  # Snowflake Snowpark profile example. See docs/Profiles.md#snowflake-snowpark.",
            "  snowflake_snowpark:",
            "    account: \"{{ env('FF_SF_ACCOUNT') }}\"",
            "    user: \"{{ env('FF_SF_USER') }}\"",
            "    password: \"{{ env('FF_SF_PASSWORD') }}\"",
            "    warehouse: \"{{ env('FF_SF_WAREHOUSE') }}\"",
            "    database: \"{{ env('FF_SF_DATABASE') }}\"",
            "    db_schema: \"{{ env('FF_SF_SCHEMA', 'PUBLIC') }}\"",
        ],
    }[ctx.engine]

    lines = [
        "# Profiles generated by `fft init`.",
        "# Update these placeholders as described in docs/Profiles.md.",
        f"{ctx.profile_name}:",
        f"  engine: {ctx.engine}",
        *engine_block,
        "",
        "# Default in-memory profile for quick experiments.",
        "default:",
        "  engine: duckdb",
        "  duckdb:",
        '    path: ":memory:"',
        "",
    ]
    return "\n".join(lines)


def _build_project_yaml(ctx: _InitContext) -> str:
    return "\n".join(
        [
            "# Project configuration generated by `fft init`.",
            "# Read docs/Project_Config.md for the complete reference.",
            f"name: {ctx.project_name}",
            'version: "0.1"',
            "models_dir: models",
            "",
            "docs:",
            "  # Adjust `dag_dir` to change where `fft dag --html` writes documentation "
            "(docs/Technical_Overview.md#documentation).",
            "  dag_dir: site/dag",
            "",
            "# Project-level variables accessible via {{ var('key') }} inside models.",
            "# Example:",
            "#   vars:",
            '#     run_date: "2024-01-01"',
            "vars: {}",
            "",
            "# Declare project-wide data quality checks under `tests`. "
            "See docs/Data_Quality_Tests.md.",
            "tests: []",
            "",
        ]
    )


def _build_sources_yaml() -> str:
    return "\n".join(
        [
            "# Source declarations describe external tables. See docs/Sources.md for details.",
            "version: 2",
            "sources:",
            "  # Example:",
            "  # - name: raw",
            "  #   schema: staging",
            "  #   tables:",
            "  #     - name: users",
            "  #       identifier: seed_users",
            "",
        ]
    )


def _write_file(path: Path, content: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(content, encoding="utf-8")


def _create_directory_notes(target: Path) -> None:
    notes = {
        "models/README.md": "\n".join(
            [
                "# Models directory",
                "",
                "Place SQL (`*.ff.sql`) and Python (`*.ff.py`) models here.",
                "See docs/Config_and_Macros.md for modeling guidance and config options.",
                "",
            ]
        ),
        "seeds/README.md": "\n".join(
            [
                "# Seeds directory",
                "",
                "Add CSV or Parquet files for reproducible seeds.",
                "Usage examples are covered in docs/Quickstart.md and "
                "docs/Config_and_Macros.md#13-seeds-sources-and-dependencies.",
                "",
            ]
        ),
        "tests/unit/README.md": "\n".join(
            [
                "# Unit tests",
                "",
                "Define YAML unit specs as described in "
                "docs/Config_and_Macros.md#73-model-unit-tests-fft-utest.",
                "Invoke them with `fft utest <project> --env <profile>`.",
                "",
            ]
        ),
        "docs/README.md": "\n".join(
            [
                "# Project documentation",
                "",
                "Write operator or contributor notes here and keep "
                "them in sync with generated docs.",
                "See docs/Technical_Overview.md#documentation "
                "for `fft dag` / `fft docgen` guidance.",
                "",
            ]
        ),
    }
    for rel, text in notes.items():
        _write_file(target / rel, text)


def _build_root_readme(ctx: _InitContext) -> str:
    return "\n".join(
        [
            "# FastFlowTransform project scaffold",
            "",
            "This project was created with `fft init`.",
            "Next steps:",
            "1. Update `profiles.yml` with real connection details (docs/Profiles.md).",
            "2. Add sources in `sources.yml` and author models "
            "under `models/` (docs/Config_and_Macros.md).",
            "3. Seed sample data with `fft seed` and execute models "
            "with `fft run` (docs/Quickstart.md).",
            "",
        ]
    )


def init(
    project_dir: Annotated[
        Path,
        typer.Argument(
            help="Directory to create (must not exist). For example: ./my_project",
        ),
    ],
    name: Annotated[
        str | None,
        typer.Option("--name", help="Project name; defaults to the target directory name."),
    ] = None,
    engine: Annotated[
        str,
        typer.Option(
            "--engine",
            help=(
                "Executor engine for the default profile. "
                "Supported values: duckdb, postgres, bigquery, bigquery_bf, "
                "databricks_spark, snowflake_snowpark."
            ),
        ),
    ] = "duckdb",
    profile_name: Annotated[
        str,
        typer.Option("--profile-name", help="Profile name to generate inside profiles.yml."),
    ] = "dev",
) -> None:
    resolved_engine = engine.lower().strip()
    if resolved_engine not in _SUPPORTED_ENGINES:
        typer.secho(
            (
                f"Unsupported engine '{engine}'. "
                "Choose one of: {', '.join(sorted(_SUPPORTED_ENGINES))}."
            ),
            fg="red",
        )
        raise typer.Exit(2)

    project_dir = project_dir.resolve()
    project_name = name or project_dir.name

    try:
        project_dir.mkdir(parents=True, exist_ok=False)
    except FileExistsError as err:
        typer.secho(
            f"Cannot initialise project: directory '{project_dir}' already exists. "
            "Choose a new path or remove the existing directory first.",
            fg="red",
        )
        raise typer.Exit(1) from err

    ctx = _InitContext(
        project_dir=project_dir,
        project_name=project_name,
        profile_name=profile_name,
        engine=resolved_engine,
    )

    for sub in ("models", "seeds", "tests/unit", "docs"):
        (project_dir / sub).mkdir(parents=True, exist_ok=True)

    _write_file(project_dir / "project.yml", _build_project_yaml(ctx))
    _write_file(project_dir / "profiles.yml", _build_profiles_yaml(ctx))
    _write_file(project_dir / "sources.yml", _build_sources_yaml())
    _write_file(project_dir / "README.md", _build_root_readme(ctx))
    _create_directory_notes(project_dir)

    typer.secho(f"âœ“ Project skeleton created at {project_dir}", fg="green")


def register(app: typer.Typer) -> None:
    app.command(
        help=(
            "Create a FastFlowTransform project skeleton (non-interactive).\n\n"
            "Examples:\n"
            "  fft init ./analytics --name analytics --engine duckdb\n"
            "  fft init ~/projects/warehouse --engine postgres --profile-name prod\n"
        )
    )(init)


__all__ = ["init", "register"]
