# profiles.yml for API demo
# Actual connection details are sourced from environment variables (preferably via .env files).

dev_duckdb:
  engine: duckdb
  vars:
    api_users_model: "api_users_http"
  duckdb:
    path: "{{ env('FF_DUCKDB_PATH', '.local/api_demo.duckdb') }}"

dev_postgres:
  engine: postgres
  vars:
    api_users_model: "api_users_http"
  postgres:
    dsn: "{{ env('FF_PG_DSN') }}"
    db_schema: "{{ env('FF_PG_SCHEMA', 'public') }}"

dev_databricks:
  engine: databricks_spark
  vars:
    api_users_model: "api_users_http"
  databricks_spark:
    master: "{{ env('FF_SPARK_MASTER', 'local[*]') }}"
    app_name: "{{ env('FF_SPARK_APP_NAME', 'api_demo') }}"
    warehouse_dir: "{{ project_dir() }}/.local/spark_warehouse"
    extra_conf:
      spark.hadoop.javax.jdo.option.ConnectionURL: "jdbc:derby:{{ project_dir() }}/.local/metastore_db;create=true"
      spark.hadoop.datanucleus.rdbms.datastoreAdapterClassName: "org.datanucleus.store.rdbms.adapter.DerbyAdapter"
      spark.hadoop.datanucleus.schema.autoCreateAll: "true"
      spark.hadoop.javax.jdo.option.ConnectionDriverName: "org.apache.derby.jdbc.EmbeddedDriver"
      spark.driver.extraJavaOptions: "-Dderby.stream.error.file={{ project_dir() }}/.local/derby.log"

default:
  engine: duckdb
  vars:
    api_users_model: "api_users_http"
  duckdb:
    path: "{{ env('FF_DUCKDB_PATH', ':memory:') }}"
