dev_duckdb:
  engine: duckdb
  duckdb:
    path: "{{ env('FF_DUCKDB_PATH', '.local/incremental_demo.duckdb') }}"

dev_postgres:
  engine: postgres
  postgres:
    dsn: "{{ env('FF_PG_DSN') }}"
    db_schema: "{{ env('FF_PG_SCHEMA', 'public') }}"

dev_databricks_parquet: &incremental_databricks_parquet
  engine: databricks_spark
  databricks_spark:
    master: "{{ env('FF_SPARK_MASTER', 'local[*]') }}"
    app_name: "{{ env('FF_SPARK_APP_NAME', 'incremental_demo_parquet') }}"
    warehouse_dir: "{{ project_dir() }}/.local/spark_warehouse_parquet"
    database: "{{ env('FF_DBR_DATABASE', 'incremental_demo_parquet') }}"
    extra_conf:
      spark.hadoop.javax.jdo.option.ConnectionURL: "jdbc:derby:{{ project_dir() }}/.local/metastore_db;create=true"
      spark.hadoop.datanucleus.rdbms.datastoreAdapterClassName: "org.datanucleus.store.rdbms.adapter.DerbyAdapter"
      spark.hadoop.datanucleus.schema.autoCreateAll: "true"
      spark.hadoop.javax.jdo.option.ConnectionDriverName: "org.apache.derby.jdbc.EmbeddedDriver"
      spark.driver.extraJavaOptions: >
        -Dderby.stream.error.file={{ project_dir() }}/.local/derby.log
        -Dderby.system.home={{ project_dir() }}/.local/derby_home

dev_databricks_delta:
  engine: databricks_spark
  databricks_spark:
    master: "{{ env('FF_SPARK_MASTER', 'local[*]') }}"
    app_name: "{{ env('FF_SPARK_APP_NAME', 'incremental_demo_delta') }}"
    warehouse_dir: "{{ project_dir() }}/.local/spark_warehouse_delta"
    database: "{{ env('FF_DBR_DATABASE', 'incremental_demo_delta') }}"
    extra_conf:
      spark.hadoop.javax.jdo.option.ConnectionURL: "jdbc:derby:{{ project_dir() }}/.local/metastore_db;create=true"
      spark.hadoop.datanucleus.rdbms.datastoreAdapterClassName: "org.datanucleus.store.rdbms.adapter.DerbyAdapter"
      spark.hadoop.datanucleus.schema.autoCreateAll: "true"
      spark.hadoop.javax.jdo.option.ConnectionDriverName: "org.apache.derby.jdbc.EmbeddedDriver"
      spark.driver.extraJavaOptions: "-Dderby.stream.error.file={{ project_dir() }}/.local/derby.log"

dev_databricks_iceberg:
  engine: databricks_spark
  databricks_spark:
    master: "{{ env('FF_SPARK_MASTER', 'local[*]') }}"
    app_name: "{{ env('FF_SPARK_APP_NAME', 'incremental_demo_iceberg') }}"
    warehouse_dir: "{{ project_dir() }}/.local/spark_warehouse_iceberg"
    database: "{{ env('FF_DBR_DATABASE', 'incremental_demo_iceberg') }}"
    table_format: "iceberg"
    extra_conf:
      spark.jars.packages: "org.apache.iceberg:iceberg-spark-runtime-4.0_2.13:1.10.0"
      # minimal local Hadoop catalog example:
      spark.sql.catalog.iceberg: "org.apache.iceberg.spark.SparkCatalog"
      spark.sql.catalog.iceberg.type: "hadoop"
      spark.sql.catalog.iceberg.warehouse: "file://{{ project_dir() }}/.local/iceberg_warehouse"
      spark.sql.catalog.iceberg.write.metadata.version-hint.enabled: "false"
      spark.sql.catalog.iceberg.read.metadata.version-hint.enabled: "false"

dev_databricks_hudi:
  engine: databricks_spark
  databricks_spark:
    master: "{{ env('FF_SPARK_MASTER', 'local[*]') }}"
    app_name: "{{ env('FF_SPARK_APP_NAME', 'incremental_demo_hudi') }}"
    warehouse_dir: "{{ project_dir() }}/.local/spark_warehouse_hudi"

    # Tell DatabricksSparkExecutor / get_spark_format_handler to use HudiFormatHandler
    table_format: "hudi"

    # Default Hudi options â€“ overridden per model via models.storage
    table_options:
      hoodie.datasource.write.recordkey.field: "id"
      hoodie.datasource.write.precombine.field: "updated_at"
      hoodie.table.name: "fct_events_hudi"

    extra_conf:
      spark.jars.packages: "org.apache.hudi:hudi-spark4.0-bundle_2.13:1.0.0"

      # Core Hudi Spark wiring
      spark.serializer: "org.apache.spark.serializer.KryoSerializer"
      spark.sql.extensions: "org.apache.spark.sql.hudi.HoodieSparkSessionExtension"
      spark.sql.catalog.spark_catalog: "org.apache.spark.sql.hudi.catalog.HoodieCatalog"
      spark.kryo.registrator: "org.apache.spark.HoodieSparkKryoRegistrar"

      # Keep warehouse on disk under the project (matches warehouse_dir)
      spark.sql.warehouse.dir: "file://{{ project_dir() }}/.local/spark_warehouse_hudi"

dev_bigquery_bigframes:
  engine: bigquery
  bigquery:
    project: "{{ env('FF_BQ_PROJECT') }}"
    dataset: "{{ env('FF_BQ_DATASET', 'incremental_demo') }}"
    location: "{{ env('FF_BQ_LOCATION', 'EU') }}"
    use_bigframes: true
    # allow_create_dataset: true   # uncomment to auto-create dataset on first run

dev_bigquery_pandas:
  engine: bigquery
  bigquery:
    project: "{{ env('FF_BQ_PROJECT') }}"
    dataset: "{{ env('FF_BQ_DATASET', 'incremental_demo') }}"
    location: "{{ env('FF_BQ_LOCATION', 'EU') }}"
    use_bigframes: false
    # allow_create_dataset: true   # uncomment to auto-create dataset on first run

dev_snowflake:
  engine: snowflake_snowpark
  snowflake_snowpark:
    account: "{{ env('FF_SF_ACCOUNT') }}"
    user: "{{ env('FF_SF_USER') }}"
    password: "{{ env('FF_SF_PASSWORD') }}"
    warehouse: "{{ env('FF_SF_WAREHOUSE', 'COMPUTE_WH') }}"
    database: "{{ env('FF_SF_DATABASE', 'EXAMPLE_DEMO') }}"
    schema: "{{ env('FF_SF_SCHEMA', 'INCREMENTAL_DEMO') }}"
    role: "{{ env('FF_SF_ROLE', '') }}"
    allow_create_schema: true

# Backwards-compatible alias for scripts referencing dev_databricks
dev_databricks:
  <<: *incremental_databricks_parquet
